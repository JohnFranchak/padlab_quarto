<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.563">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Perception, Action, and Development Lab -</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

<script src="site_libs/core-js-2.5.3/shim.min.js"></script>
<script src="site_libs/react-17.0.0/react.min.js"></script>
<script src="site_libs/react-17.0.0/react-dom.min.js"></script>
<script src="site_libs/reactwidget-1.0.0/react-tools.js"></script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="site_libs/reactable-binding-0.3.0/reactable.js"></script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="./index.html">
    <img src="./images/icon-plain.png" alt="">
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./Publications.html" aria-current="page">Publications</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./People.html">People</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Contact.html">Contact</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#publications" id="toc-publications" class="nav-link active" data-scroll-target="#publications">Publications</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">



<section id="publications" class="level1 column-page-inset">
<h1>Publications</h1>
<div class="cell">

</div>
<div class="cell">
<div class="cell-output-display">

<div id="cars-select" class="reactable html-widget" style="width:auto;height:auto;"></div>
<script type="application/json" data-for="cars-select">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"authors":["Franchak, J.M., & Kadooka, K.","Franchak, J.M., Scott, V., & Luo, C.","Franchak, J.M., McGee, B., & Blanch, G.","Feghhi, I., Franchak, J.M., & Rosenbaum. D.","Gagnon, H.C., Rohovit, T., Finney, H., Zhao, Y., Franchak, J.M., Stefanucci, J.K., Creem- Regehr, S.H., & Bodenheimer, R.E.","Luo, C., & Franchak, J.M.","Franchak, J.M., & Yu, C.","Kadooka, K., & Franchak, J.M.","Franchak, J.M.","Franchak, J.M."],"year":[2022,2021,2021,2021,2021,2020,2022,2020,2020,2020],"url":["2022-FranchakKadooka-Infancy.pdf","2021-FranchakScottLuo-Frontiers.pdf","2021-FranchakMcGeeBlanch-PLoSOne",null,"2021-Gagnonetal-IEEEVR.pdf","2020-LuoFranchak-PLOSOne.pdf","2022-FranchakYu-Advances.pdf","2020-KadookaFranchak-DP.pdf","2020-Franchak-PLM.pdf","2020-Franchak-QJEP.pdf"],"citation":["Franchak, J.M., & Kadooka, K. (2022). Age differences in orienting to faces in dynamic scenes depend on face centering, not visual saliency. Infancy.","Franchak, J.M., Scott, V., & Luo, C. (2021). A contactless method for measuring full- day, naturalistic motor behavior using wearable inertial sensors. Frontiers in Psychology.","Franchak, J.M., McGee, B., & Blanch, G. (2021). Adapting the coordination of eyes and head to differences in task and environment during fully-mobile visual exploration. PLoS One.","Feghhi, I., Franchak, J.M., & Rosenbaum. D. (2021). Towards a common code for difficulty: Navigating a narrow gap is like memorizing an extra digit. Attention, Perception, & Psychophysics.","Gagnon, H.C., Rohovit, T., Finney, H., Zhao, Y., Franchak, J.M., Stefanucci, J.K., Creem- Regehr, S.H., & Bodenheimer, R.E. (2021). The effect of feedback on estimates of reaching ability in virtual reality. Proceedings of the 2021 IEEE Virtual Reality (VR), Lisbon, Portugal.","Luo, C., & Franchak, J.M. (2020). Head and body structure infants’ visual experiences during mobile, naturalistic play. PLoS One.","Franchak, J.M., & Yu, C. (2022). Beyond screen time: Using head-mounted eye tracking to study natural behavior. Advances in Child Development and Behavior, Vol 62.","Kadooka, K., & Franchak, J.M. (2020). Developmental changes in infants' and children's attention to faces and salient regions vary across and within video stimuli. Developmental Psychology.","Franchak, J.M. (2020). Visual exploratory behavior and its development. The Psychology of Learning and Motivation (Vol. 73): Gazing toward the future: Advances in eye movement theory and applications.","Franchak, J.M. (2020). Calibration of perception fails to transfer between functionally similar affordances. Quarterly Journal of Experimental Psychology."],"type":["Journal","Journal","Journal","Journal","Proceedings","Journal","Chapter","Journal","Chapter","Journal"],"doi":[null,null,null,"https://doi.org/10.3758/s13414-021-02356-4",null,null,null,null,null,null],"inpress":[1,0,0,0,0,0,0,0,0,0]},"columns":[{"accessor":"authors","name":"","type":"character","show":false},{"accessor":"year","name":"","type":"numeric","show":false},{"accessor":"url","name":"","type":"character","show":true,"filterable":false,"cell":[{"name":"a","attribs":{"href":"publications/2022-FranchakKadooka-Infancy.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2021-FranchakScottLuo-Frontiers.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2021-FranchakMcGeeBlanch-PLoSOne","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/NA","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2021-Gagnonetal-IEEEVR.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2020-LuoFranchak-PLOSOne.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2022-FranchakYu-Advances.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2020-KadookaFranchak-DP.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2020-Franchak-PLM.pdf","target":"_blank"},"children":["↓"]},{"name":"a","attribs":{"href":"publications/2020-Franchak-QJEP.pdf","target":"_blank"},"children":["↓"]}],"maxWidth":20},{"accessor":"citation","name":"","type":"character","show":true,"filterable":false,"details":["Abstract:  The current study investigated how infants (6-24 months), children (2-12 years), and adults differ in how visual cues—visual saliency and centering—guide their attention to faces in videos. We report a secondary analysis of Kadooka and Franchak (2020), in which observers’ eye movements were recorded during viewing of television clips containing a variety of faces. For every face on every video frame, we calculated its visual saliency (based on both static and dynamic image features) and calculated how close the face was to the center of the image. Results revealed that participants of every age looked more often at each face when it was more salient compared to less salient. In contrast, centering did not increase the likelihood that infants looked at a given face, but in later childhood and adulthood centering became a stronger cue for face looking. A control analysis determined that the age-related change in centering was specific to face looking; participants of all ages were more likely to look in the center of the image, and this center bias did not change with age. The implications for using videos in educational and diagnostic contexts are discussed.","Abstract:  How can researchers best measure infants' motor experiences in the home? Body position—whether infants are held, supine, prone, sitting, or upright—is an important developmental experience. However, the standard way of measuring infant body position, video recording by an experimenter in the home, can only capture short instances, may bias measurements, and conflicts with physical distancing guidelines resulting from the COVID-19 pandemic. Here, we introduce and validate an alternative method that uses machine learning algorithms to classify infants' body position from a set of wearable inertial sensors. A laboratory study of 15 infants demonstrated that the method was sufficiently accurate to measure individual differences in the time that infants spent in each body position. Two case studies showed the feasibility of applying this method to testing infants in the home using a contactless equipment drop-off procedure.","Abstract:  How are eyes and head adapted to meet the demands of visual exploration in different tasks and environments? In two studies, we measured the horizontal movements of the eyes (using mobile eye tracking in Studies 1 and 2) and the head (using inertial sensors in Study 2) while participants completed a walking task and a search and retrieval task in a large, outdoor environment. We found that the spread of visual exploration was greater while searching compared with walking, and this was primarily driven by increased movement of the head as opposed to the eyes. The contributions of the head to gaze shifts of different eccentricities was greater when searching compared to when walking. Findings are discussed with respect to understanding visual exploration as a motor action with multiple degrees of freedom.","Abstract:  What makes a task hard or easy? The question seems easy, but answering it has been hard. The only consensus has been that, all else being equal, easy tasks can be performed by more individuals than hard tasks, and easy tasks are usually preferred over hard tasks. Feghhi and Rosenbaum (Journal of Experimental Psychology: Human Perception and Performance, 45, 983–994, 2019) asked whether task difficulty might reflect a single amodal quantity. Based on their subjects’ two-alternative forced-choice data from tasks involving choices of tasks with graded physical and mental challenges, the authors showed that the difficulty of passing through a narrow gap rather than a wide gap was psychologically equivalent to memorizing an extra .55 digits. In the present study, we extended this approach by adding new arguments for the hypothesis that task difficulty might reflect a single amodal quantity (inspired by considerations of physics, economics, and the common code hypothesis for the study of perception and action), and we tested narrower gaps than before to see whether we would find a larger equivalent memory-digit. Consistent with our prediction, we obtained a value of .95. We suggest that our multi-modal two-alternative forced-choice procedure can pave the way toward a better understanding of task difficulty.","Abstract:  Immersive virtual environments (VEs) are most useful for training and education when viewers perceive and act accurately within them. Judgments of action capabilities within a VE provide a good measure of perceptual fidelity — the notion of how closely perception and action in the VE match that in the real world — and can also assess how perception for action may be calibrated with visual feedback based on one’s own actions. In the current study we tested judg- ments of action capabilities within a VE for two different reaching behaviors: reaching out and reaching up. Our goal was to assess whether feedback from actual reaching improves judgments and if any recalibration due to feedback differed across reaching behaviors. We first measured participants’ actual reaching out and reaching up capabilities so that feedback trials could be scaled to their actual abil- ities. Participants then completed blocks of alternating perceptual adjustment and feedback trials. In adjustment trials, they adjusted a virtual target to a distance perceived to be just reachable. In feedback trials, they viewed targets that were farther or closer than their actual reach, decided whether the target was reachable, and then reached out to the target to receive visual feedback from a hand-held con- troller. The first feedback block manipulated the target distance to be 30% over or under actual reach and subsequent blocks decreased the deviation to 20%, 10% and 5% of actual reach. We found that for both reaching behaviors, reach was initially overestimated, and then perceptual estimations decreased to become more accurate over feed- back blocks. Accuracy in the feedback trials themselves showed that targets just beyond reach were more difficult to judge correctly. This study establishes a straightforward methodology that can be used for calibration of actions in VEs and has implications for applications that depend on accurate reaching within VEs.","Abstract:  Infants’ visual experiences are important for learning, and may depend on how information is structured in the visual field. This study examined how objects are distributed in 12-month-old infants’ field of view in a mobile play setting. Infants wore a mobile eye tracker that recorded their field of view and eye movements while they freely played with toys and a caregiver. We measured how centered and spread object locations were in infants’ field of view, and investigated how infant posture, object looking, and object distance affected the centering and spread. We found that far toys were less centered in infants’ field of view while infants were prone compared to when sitting or upright. Overall, toys became more centered in view and less spread in location when infants were looking at toys regardless of posture and toy distance. In sum, this study showed that infants’ visual experiences are shaped by the physical relation between infants’ bodies and the locations of objects in the world. However, infants are able to compensate for postural and environmental constraints by actively moving their head and eyes when choosing to look at an object.","Abstract:  Head-mounted eye tracking is a new method that allows researchers to catch a glimpse of what infants and children see during naturalistic activities. In this chapter, we review how mobile, wearable eye trackers improve the construct validity of important developmental constructs, such as visual object experiences and social attention, in ways that would be impossible using screen-based eye tracking. Head-mounted eye tracking improves ecological validity by allowing researchers to present more realistic and complex visual scenes, create more interactive experimental situations, and examine how the body influences what infants and children see. As with any new method, there are difficulties to overcome. Accordingly, we identify what aspects of head-mounted eye tracking study design affect the measurement quality, interpretability of the results, and efficiency of gathering data. Moreover, we provide a summary of best practices aimed at allowing researchers to make well-informed decisions about whether and how to apply head-mounted eye tracking to their own research questions.","Abstract:  Visual attention in complex, dynamic scenes is attracted to locations that contain socially relevant features, such as faces, and to areas that are visually salient. Previous work suggests that there is a global shift over development such that observers increasingly attend to faces with age. However, no prior work has tested whether this shift is truly global, that is, consistent across and within stimuli despite variations in content. To test the global shift hypothesis, we recorded eye movements of 89 children (6 months to 10 years) and adults while they viewed 7 video clips. We measured the extent to which each participant attended to faces and to salient areas for each video. There was no evidence of global age-related changes in attention: Neither feature showed consistent increases or decreases with age. Moreover, windowed analyses within each stimulus video revealed significant moment-to-moment variations in the relation between age and each visual feature (via a bootstrapping analysis). For some time windows, adults looked more often at both feature types compared to infants and children. However, for other time windows, the pattern was reversed—younger participants looked more at faces and salient locations. Lack of consistent directional effects provides strong evidence against the global shift hypothesis. We suggest an alternative explanation: Over development, observers increasingly prioritize when and where to look by learning to track which features are relevant within a scene. Implications for the development of visual attention and children’s understanding of screen-based media are discussed.","Abstract:  Visual exploratory behavior refers to actively gathering visual information through coordinated eye, head, and body movements—looking around with a purpose. In this article, I compare the study of visual exploration using stationary, screen-based tasks versus mobile, naturalistic tasks. In doing so, I discuss how the ecological validity of research depends on variations in three factors: visual characteristics of stimuli, the opportunity for participants to interact with their surroundings, and participants’ ability to move their bodies. In particular, the long-standing reliance on stationary, screen tasks has precluded progress in understanding the last factor—how anatomical, biomechanical, and physiological aspects of the body influence where observers look. I argue that each factor is vital to revealing the flexibility, adaptiveness, and efficiency of everyday visual exploration.","Abstract:  Prior work shows that the calibration of perception and action transfers between actions depending on their functional similarity: Practising (and thus calibrating perception of) one affordance will also calibrate perception for an affordance with a similar function but not for an affordance with a disparate function. We tested this hypothesis by measuring whether calibration transferred between two affordances for passing through openings: squeezing sideways through doorways without becoming stuck and fitting sideways through doorways while avoiding collision. Participants wore a backpack to alter affordances for passage and create a need for perceptual recalibration. Calibration failed to transfer between the two actions (e.g., practising squeezing through doorways calibrated perception of squeezing but not fitting). Differences between squeezing and fitting affordances that might have required different information for perception and recalibration are explored to understand why calibration did not transfer. In light of these results, we propose a revised hypothesis—calibration transfers between affordances on the basis of both functional and informational similarity."],"minWidth":350},{"accessor":"type","name":"","type":"character","show":true,"filterInput":{"name":"select","attribs":{"aria-label":"Filter type","style":{"width":"100%","height":"28px"},"onChange":"function(_e){(function(event){Reactable.setFilter('cars-select', 'type', event.target.value || undefined)}).apply(event.target,[_e])}"},"children":[{"name":"option","attribs":{"value":""},"children":["All"]},{"name":"option","attribs":{},"children":["Journal"]},{"name":"option","attribs":{},"children":["Proceedings"]},{"name":"option","attribs":{},"children":["Chapter"]}]},"maxWidth":115},{"accessor":"doi","name":"","type":"character","show":false},{"accessor":"inpress","name":"","type":"numeric","show":false}],"filterable":true,"searchable":true,"defaultSortDesc":true,"defaultSorted":[{"id":"year","desc":true},{"id":"authors","desc":true}],"defaultPageSize":10,"paginationType":"numbers","showPageInfo":true,"minRows":1,"theme":{"color":"#222222","backgroundColor":"transparent","stripedColor":"lightgrey","highlightColor":"lightgrey","cellPadding":6,"tableStyle":{"fontSize":20},"headerStyle":{"color":"#222222","borderColor":"transparent","fontSize":15},"cellStyle":{"borderColor":"transparent"},"selectStyle":{"color":"transparent"},"paginationStyle":{"color":"transparent","borderColor":"transparent"}},"elementId":"cars-select","dataKey":"8531cf019024d51b31376fd956552306"},"children":[]},"class":"reactR_markup"},"evals":["tag.attribs.columns.4.filterInput.attribs.onChange"],"jsHooks":[]}</script>
</div>
</div>
</section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>